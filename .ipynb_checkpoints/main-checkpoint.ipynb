{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Madrid pollution data extraction and cleaning**\n",
    "\n",
    "Goal:\n",
    "This project goal is to extract public data information about pollution in Madrid, clean it and add data that makes sense if there are missing values, using Python programming language. The source is this Madrid council webpage. We will download data for 2018. This data is real time hourly data. In this case we will just keep the NO2 agent.\n",
    "\n",
    "Steps:\n",
    "In this Jupyter Notebook the next steps are taken:\n",
    "\n",
    "Downloading the data (2018 real time data url), which comes in a zip folder with a file for each month in 3 different formats (.txt, .csv and .xml). In this case we will use the .csv extension files.\n",
    "\n",
    "Extracting just the .csv files from the zip folder.\n",
    "\n",
    "Adding records for missing days.\n",
    "\n",
    "Transforming the monthly dataframe, as the default format is not good to work with. There is a row for each day, with a column for each hour in the day, and another column for each hour to tell if that value is validated or not. In the transformed dataframe, we will have a column for hours and another one to indicate if that record is validated or not.\n",
    "\n",
    "Filling non validated hour values. We assign the average of the previous validated value and the next validated value.\n",
    "\n",
    "Adding columns for year-month-date and year-month-date-time.\n",
    "\n",
    "Finally we append all the monthly files together to have a yearly file.\n",
    "\n",
    "Some tests to check if the transformation has been satisfactory.\n",
    "\n",
    "Some data exploration.\n",
    "\n",
    "Some charts to see visualized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_url(url, file_name):\n",
    "    '''Downloads data from a url and stores it in the location where it is executed.'''\n",
    "    urllib.request.urlretrieve(url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_zip_folder(file_type, zip_folder_path, new_folder_path):\n",
    "    '''This function extracts files from a zip folder.\n",
    "    It just extracts the files of a certain type\n",
    "    '''\n",
    "    with ZipFile(zip_folder_path, 'r') as zip_obj:\n",
    "        # Gets a list of all archived file names from the zip.\n",
    "        list_of_file_names = zip_obj.namelist()\n",
    "        # Iterates over the file names.\n",
    "        for file_name in list_of_file_names:\n",
    "            # Checks filename endswith the required file type.\n",
    "            if file_name.endswith(f'.{file_type}'):\n",
    "                # Extracts a single file from zip.\n",
    "                print(f'extracting {file_name}')\n",
    "                zip_obj.extract(file_name, new_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_missing_days(dataframe, partition_column):\n",
    "    '''This function takes the monthly dataframe and checks if there are missing days\n",
    "    for a specific sample spot. If so, it appends a row to the original dataframe with\n",
    "    the info of that day and the validation columns set to N (non validated), to know that\n",
    "    info is not correct (we will correct it later).\n",
    "    '''\n",
    "    year = dataframe.loc[0, 'ANO']\n",
    "    month = dataframe.loc[0, 'MES']\n",
    "    \n",
    "    # First we have to know how many days a specific month has.\n",
    "    _, number_days_month = calendar.monthrange(year, month)\n",
    "    \n",
    "    # We create a list with all the days of that month.\n",
    "    list_of_days_of_the_month = list(range(1, number_days_month + 1))\n",
    "    \n",
    "    # We create a list with all the sample spots.\n",
    "    sample_spots_list = list(set(dataframe[partition_column]))\n",
    "    \n",
    "    for sample_spot in sample_spots_list:\n",
    "        # We create a df with just the info of one spot.\n",
    "        sample_spot_df = dataframe[\n",
    "            dataframe[partition_column] == sample_spot].reset_index(drop=True)\n",
    "\n",
    "        # We check if all that days are contained in the spot df.\n",
    "        isin_df = pd.Series(list_of_days_of_the_month).isin(list(sample_spot_df['DIA']))\n",
    "        isin_df.index = list_of_days_of_the_month\n",
    "\n",
    "        # Now, if a day is not included, we append a row with its data to the original df.\n",
    "        for day, isin in isin_df.iteritems():\n",
    "            if isin == False:\n",
    "                print(f'Day {day}-{month}-{year} missing')\n",
    "                \n",
    "                # We take the first row of the df, but we change the day and the\n",
    "                # validation columns to 'N'.\n",
    "                row_to_append = [\n",
    "                    sample_spot_df.loc[0, column] for column in sample_spot_df.columns]\n",
    "                row_to_append[6] = day\n",
    "                for i, e in enumerate(row_to_append):\n",
    "                    if e == 'V':\n",
    "                        row_to_append[i] = 'N'\n",
    "                \n",
    "                # We append the row.\n",
    "                dataframe = dataframe.append(\n",
    "                    pd.Series(row_to_append, index=sample_spot_df.columns),\n",
    "                    ignore_index=True\n",
    "                )\n",
    "                \n",
    "                print(f'Day {day}-{month}-{year} row added to original dataframe')\n",
    "            \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacked_dataframe(dataframe, cols_to_drop, cols_remain):\n",
    "    '''This function applies the pandas stack method to make data that is\n",
    "    spread in columns collapse in a single column.\n",
    "    First drops the columns that would not let the stack work properly,\n",
    "    as we want to have the columns that will remain as they are,\n",
    "    and the columns that will be stacked in the same column.\n",
    "    Then sets the columns that do not have to be stacked as the index.\n",
    "    Applies stack method. Finally, resets index.\n",
    "    '''\n",
    "    dataframe = dataframe.drop(columns=cols_to_drop).set_index(cols_remain)\n",
    "    dataframe = dataframe.stack().reset_index()\n",
    "    dataframe = dataframe[dataframe[dataframe.columns[-2]] != 'index']\n",
    "    dataframe = dataframe.reset_index(drop=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_last_col_to_df(df1, df2):\n",
    "    ''' Adds the last column from a dataframe to another dataframe\n",
    "    with the same number of rows.\n",
    "    '''\n",
    "    df1['new_col'] = df2.iloc[:,-1]\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_validated_value(dataframe, index, column, get_next=True):\n",
    "    '''This function gets the nearest next or previous validated row index in a dataframe.\n",
    "    If the get_next param is set to True, it looks for the nearest next validated row index,\n",
    "    and if set to False, looks for the nearest previous validated row index.\n",
    "    '''\n",
    "    # We set a default initial values to start the iteration.\n",
    "    iterator = 0\n",
    "    next_validated = 'N'\n",
    "    \n",
    "    # The loop starts. It checks if the next (or previous) row value is V.\n",
    "    # If it is V, the while loop breaks and returns the required index.\n",
    "    # If it is not V, it checks the next (or previous) row, and so on until it finds a V.\n",
    "    while next_validated != 'V':\n",
    "        iterator += 1\n",
    "        \n",
    "        if get_next:\n",
    "            next_validated = dataframe.loc[index + iterator, column]\n",
    "            wanted_index = index + iterator\n",
    "        else:\n",
    "            next_validated = dataframe.loc[index - iterator, column]\n",
    "            wanted_index = index - iterator\n",
    "\n",
    "    return wanted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_non_validated_values(dataframe, partition_column):\n",
    "    '''This function partitions the dataframe by a partition column (PUNTO_MUESTREO)\n",
    "    into smaller dataframes and, for each of them, sets new values to non validated records,\n",
    "    based on near values. It tries to get the nearest next and previous validated values,\n",
    "    to assign an average of them. If it doesn't find a nearest next validated value,\n",
    "    it assigns the nearest previous one, and viceversa.\n",
    "    '''\n",
    "    # First we sort the data.\n",
    "    dataframe = dataframe.sort_values(by=['PUNTO_MUESTREO', 'ANO', 'MES', 'DIA', 'HORA'])\n",
    "    dataframe = dataframe.reset_index(drop=True)\n",
    "    \n",
    "    # We get a list of the partition_column unique values, to loop through them.\n",
    "    sample_spots_list = list(set(dataframe[partition_column]))\n",
    "    \n",
    "    # We start the loop.\n",
    "    for sample_spot in sample_spots_list:\n",
    "        \n",
    "        # We get the partitioned dataframe.\n",
    "        sample_spot_df = dataframe[dataframe[partition_column] == sample_spot]\n",
    "        \n",
    "        # Now we assign the non validated records to a new dataframe.\n",
    "        sample_spot_df_n = sample_spot_df[sample_spot_df['VALIDADO'] == 'N']\n",
    "        \n",
    "        # We loop over the non validated records dataframe.\n",
    "        for index, row in sample_spot_df_n.iterrows():            \n",
    "            \n",
    "            # We try to get the next nearest validated value in the original df.\n",
    "            try:\n",
    "                next_validated_index = get_next_validated_value(\n",
    "                    sample_spot_df, index, 'VALIDADO')\n",
    "                \n",
    "                # Now we try to get the previous nearest validated value.\n",
    "                try:\n",
    "                    previous_validated_index = get_next_validated_value(\n",
    "                        sample_spot_df, index, 'VALIDADO', get_next=False)\n",
    "                    dataframe.loc[index, 'NIVEL_NO2'] = (\n",
    "                        sample_spot_df.loc[next_validated_index, 'NIVEL_NO2'] +\n",
    "                        sample_spot_df.loc[previous_validated_index, 'NIVEL_NO2']\n",
    "                    ) / 2\n",
    "                \n",
    "                # I we reach this point, it means that there are validated values in\n",
    "                # the next rows, but there are not any validated values in the\n",
    "                # previous rows, so we assign the nearest next validated value.\n",
    "                except KeyError:\n",
    "                    next_validated_index = get_next_validated_value(\n",
    "                        sample_spot_df, index, 'VALIDADO')\n",
    "                    dataframe.loc[index, 'NIVEL_NO2'] = sample_spot_df.loc[\n",
    "                        next_validated_index, 'NIVEL_NO2']\n",
    "            \n",
    "            # I we reach this point, it means that there are validated values in the\n",
    "            # previous rows, but there are not any validated values in the next rows,\n",
    "            # so we assign the nearest previous validated value.\n",
    "            except KeyError:\n",
    "                next_validated_index = get_next_validated_value(\n",
    "                    sample_spot_df, index, 'VALIDADO', get_next=False)\n",
    "                dataframe.loc[index, 'NIVEL_NO2'] = sample_spot_df.loc[\n",
    "                    next_validated_index, 'NIVEL_NO2']\n",
    "                \n",
    "            # For now, we asume that there will always be at least either a\n",
    "            # next validated value or a previous validated value.\n",
    "            \n",
    "            # So far, the 'VALIDADO' column can contain either V if it is a validated\n",
    "            # value or an N if it is not. To confirm that we have assigned a value that\n",
    "            # we think makes sense, we change that value to an R (Reassign).\n",
    "            dataframe.loc[index, 'VALIDADO'] = 'R'\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_date_to_string(integer):\n",
    "    '''It turns 1 to 9 integers into strings with a '0' before, and keeps\n",
    "    the same the rest of numbers.\n",
    "    '''\n",
    "    if len(str(integer)) == 1:\n",
    "        return '0' + str(integer)\n",
    "    else:\n",
    "        return str(integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_df(dataframe):\n",
    "    '''Gets a df, keeps just the NO2 info, splits it into 2 dataframes,\n",
    "    each of them with one of the columns that we want to stack,\n",
    "    joins them into a single dataframe, renames columns and formats HORA column.\n",
    "    The result is a much easier to use dataframe'''\n",
    "        \n",
    "    print('Keeping just NO2 data.')\n",
    "    dataframe = dataframe[dataframe['MAGNITUD'] == 8].drop(columns=['MAGNITUD'])\n",
    "    dataframe = dataframe.reset_index(drop=True)\n",
    "        \n",
    "    print('Adding missing days rows.')\n",
    "    # We need a list of the sample spots.\n",
    "    list_of_sample_spots = list(set(dataframe['PUNTO_MUESTREO']))\n",
    "    \n",
    "    # We apply the function that add records of missing days.\n",
    "    dataframe = add_missing_days(dataframe, 'PUNTO_MUESTREO')\n",
    "        \n",
    "    cols_dimensiones = ['PROVINCIA', 'MUNICIPIO', 'ESTACION',\n",
    "                        'PUNTO_MUESTREO', 'ANO', 'MES', 'DIA']\n",
    "    \n",
    "    print('Stacking dataframes.')\n",
    "    df_h = get_stacked_dataframe(\n",
    "        dataframe, cols_remain=cols_dimensiones,\n",
    "        cols_to_drop=[col for col in list(dataframe.columns) if col[0] == 'V']\n",
    "    )\n",
    "    \n",
    "    df_v = get_stacked_dataframe(\n",
    "        dataframe, cols_remain=cols_dimensiones,\n",
    "        cols_to_drop=[col for col in list(dataframe.columns) if col[0] == 'H']\n",
    "    )\n",
    "    \n",
    "    print('Joining dataframes.')\n",
    "    final_df = add_last_col_to_df(df_h, df_v)\n",
    "    \n",
    "    print('Renaming columns.')\n",
    "    final_df = final_df.rename(columns={'level_7': 'HORA', 0: 'NIVEL_NO2',\n",
    "                                        'new_col': 'VALIDADO'})\n",
    "    \n",
    "    print('Formatting HORA column.')\n",
    "    final_df['HORA'] = final_df['HORA'].apply(lambda x: int(x[-2:]))\n",
    "    \n",
    "    print('Assigning values to non validated records.')\n",
    "    final_df = assign_non_validated_values(final_df, 'PUNTO_MUESTREO')\n",
    "    \n",
    "    print('Adding date and time columns.')\n",
    "    final_df['ANO_MES_DIA'] = (\n",
    "        final_df['ANO'].astype(str) +\n",
    "        final_df['MES'].apply(int_date_to_string) +\n",
    "        final_df['DIA'].apply(int_date_to_string)\n",
    "    ).astype(int)\n",
    "    \n",
    "    final_df['ANO_MES_DIA_HORA'] = (\n",
    "        final_df['ANO'].astype(str) +\n",
    "        final_df['MES'].apply(int_date_to_string) +\n",
    "        final_df['DIA'].apply(int_date_to_string) +\n",
    "        final_df['HORA'].apply(int_date_to_string)\n",
    "    ).astype(int)\n",
    "    \n",
    "    print('Reordering columns.')\n",
    "    final_df = final_df[[\n",
    "        'PROVINCIA', 'MUNICIPIO', 'ESTACION', 'PUNTO_MUESTREO', 'ANO', 'MES', 'DIA',\n",
    "        'HORA', 'ANO_MES_DIA', 'ANO_MES_DIA_HORA', 'NIVEL_NO2', 'VALIDADO'\n",
    "    ]]\n",
    "    \n",
    "    print(f'Monthly dataframe shape: {final_df.shape}.')\n",
    "    print()\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complete_dataframe(url, file_name, desired_data_path, file_type):\n",
    "    '''This last function of the process executes the other built functions.\n",
    "    It downloads the zip folder, extracts just the required files by format,\n",
    "    accesses each of the files, transforms them in pandas dataframes, cleans, adds\n",
    "    missing values, and appends all the dataframes together.\n",
    "    '''\n",
    "    \n",
    "    # We download the file with the data.\n",
    "    download_from_url(url, file_name)\n",
    "    \n",
    "    # Extraction of just the wanted files (.csv).\n",
    "    extract_from_zip_folder(file_type, file_name, desired_data_path)\n",
    "    \n",
    "    # We create an empty dataframe to append the clean ones to it in each loop iteration.\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    # We iterate through the files.\n",
    "    for file_name in os.listdir(desired_data_path):\n",
    "        print()\n",
    "        print(f'Working with {file_name}')\n",
    "        \n",
    "        # We convert the csv files into dataframes.\n",
    "        \n",
    "        # For Windows:\n",
    "        try:\n",
    "            monthly_data = pd.read_csv(f'{desired_data_path}/{file_name}', sep=';')\n",
    "        \n",
    "        # For Linux:\n",
    "        except:\n",
    "            monthly_data = pd.read_csv(f'{desired_data_path}\\\\{file_name}', sep=';')\n",
    "\n",
    "        # We pass the monthly dataframe to the cleaning function.\n",
    "        monthly_data = get_clean_df(monthly_data)\n",
    "        \n",
    "        # We append the clean monthly dataframe to the final dataframe.\n",
    "        data = data.append(monthly_data, ignore_index=True)\n",
    "        \n",
    "    print(f'Final dataframe shape: {data.shape}')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final execution.\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    url = 'https://datos.madrid.es/egob/catalogo/201200-10306314-calidad-aire-horario.zip'\n",
    "    file_name = 'pollution_data.zip'\n",
    "    file_type = 'csv'\n",
    "    desired_data_path = f'pollution_data_{file_type}'\n",
    "\n",
    "    final_df = get_complete_dataframe(url, file_name, desired_data_path, file_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
